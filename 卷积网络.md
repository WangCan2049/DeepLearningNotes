卷积网络
---

卷积网络（ convolutional network） (LeCun, 1989)，也叫做 卷积神经网络（ convolutional neural network, CNN），是一种专门用来处理具有类似网格结构的数据的神经网络。

## 卷积运算
s(t) = ∫x(a)w(t−a)da
卷积运算通常用星号表示：
s(t) = (x ∗ w)(t)

在卷积网络的术语中，卷积的第一个参数（在这个例子中，函数 x）通常叫做 输入（ input），第二个参数（函数 w）叫做 核函数（ kernel function）。输出有时被称作 特征映射（ feature map）。

## 动机
卷积运算通过三个重要的思想来帮助改进机器学习系统： 稀疏交互（ sparse interactions）、 参数共享（ parameter sharing）、 等变表示（ equivariant representations）。另外，卷积提供了一种处理大小可变的输入的方法。

传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵中每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。

卷积网络具有 稀疏交互（ sparse interactions）（也叫做 稀疏连接（ sparse connectivity）或者 稀疏权重（ sparse weights））的特征。这是使核的大小远小于输入的大小来达到的。

参数共享（ parameter sharing）是指在一个模型的多个函数中使用相同的参数。

卷积在存储需求和统计效率方面极大地优于稠密矩阵的乘法运算。

## 池化
卷积网络中一个典型层包含三级。
在第一级中，这一层并行地计算多个卷积产生一组线性激活响应。
在第二级中，每一个线性激活响应将会通过一个非线性的激活函数，例如整流线性激活函数。这一级有时也被称为 探测级（ detector stage）。
在第三级中，我们使用 池化函数（ pooling function）来进一步调整这一层的输出。

池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。

不管采用什么样的池化函数，当输入作出少量平移时， 池化能够帮助输入的表
示近似 不变（ invariant）。

局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现
而不关心它出现的具体位置时。

## 卷积与池化作为一种无限强的先验
 先验概率分布（ prior probability distribution）的概念:
 这
是一个模型参数的概率分布，它刻画了在我们看到数据之前我们认为什么样的模型
是合理的信念。
先验被认为是强或者弱取决于先验中概率密度的集中程度。弱先验具有较高的
熵值，例如方差很大的高斯分布。这样的先验允许数据对于参数的改变具有或多或
少的自由性。强先验具有较低的熵值，例如方差很小的高斯分布。这样的先验在决
定参数最终取值时起着更加积极的作用。

总之，我们可以把卷积的使用当作是
对网络中一层的参数引入了一个无限强的先验概率分布。这个先验说明了该层应该
学得的函数只包含局部连接关系并且对平移具有等变性。


？？？先验？？？

## 基本卷积函数的变体


## 结构化输出
卷积神经网络可以用于输出高维的结构化对象，而不仅仅是预测分类任务的类
标签或回归任务的实数值。通常这个对象只是一个张量，由标准卷积层产生。

## 数据类型


## 高效的卷积算法
卷积等效于使用傅立叶变换将输入与核都转换到频域、执行两个信号的逐点相
乘，再使用傅立叶逆变换转换回时域。对于某些问题的规模，这种算法可能比离散
卷积的朴素实现更快。

当一个 d 维的核可以表示成 d 个向量（每一维一个向量）的外积时，该核被称
为 可分离的（ separable）。


## 随机或无监督的特征
通常， 卷积网络训练中最昂贵的部分是学习特征。输出层的计算代价通常相对
不高，因为在通过若干层池化之后作为该层输入的特征的数量较少。当使用梯度下
降执行监督训练时，每步梯度计算需要完整地运行整个网络的前向传播和反向传播。
减少卷积网络训练成本的一种方式是使用那些不是由监督方式训练得到的特征。


## 卷积网络的神经科学基础

