序列建模：循环和递归网络
---


循环神经网络（ recurrent neural network）或 RNN (Rumelhart et al., 1986c)
是一类用于处理序列数据的神经网络。

## 展开计算图
计算图是形式化一组计算结构的方式，如那些涉及将输入和参数映射到输出和
损失的计算。

应用于展开图且代价为 O(τ) 的反向
传播算法称为 通过时间反向传播（ back-propagation through time, BPTT）

## 循环神经网络

### 导师驱动过程和输出循环网络
由输出反馈到模型而产生循环连接的模型可用 导师驱动过程（ teacher forcing）
进行训练。

### 作为有向图模型的循环网络


### 基于上下文的 RNN 序列建模


### 双向 RNN


## 基于编码-解码的序列到序列架构

（ 1） 编码
器（ encoder）或 读取器 (reader) 或 输入 (input) RNN 处理输入序列。 编码器输出
上下文 C（通常是最终隐藏状态的简单函数）。 (2) 解码器（ decoder）或 写入器
(writer) 或 输出 (output) RNN 则以固定长度的向量（如图 10.9 ）为条件产生输出
序列 Y = (y(1); : : : ; y(ny))。

## 深度循环网络
大多数 RNN 中的计算可以分解成三块参数及其相关的变换：
1. 从输入到隐藏状态，
2. 从前一隐藏状态到下一隐藏状态，以及
3. 从隐藏状态到输出。

## 递归神经网络
递归神经网络代表循环网络的另一个扩展，它被构造为深的树状结构而不
是 RNN 的链状结构，因此是不同类型的计算图。

递归网络已成功地应用于输入是数据结构的神经网络 (Frasconi
et al., 1997, 1998)，如自然语言处理 (Socher et al., 2011a,c, 2013a) 和计算机视觉
(Socher et al., 2011b)。

递归网络的一个明显优势是，对于具有相同长度 τ 的序列，深度（通过非线性
操作的组合数量来衡量）可以急剧地从 τ 减小为 O(log τ)，这可能有助于解决长期依赖。一个悬而未决的问题是如何以最佳的方式构造树。一种选择是使用不依赖于
数据的树结构，如平衡二叉树。在某些应用领域，外部方法可以为选择适当的树结构
提供借鉴。

## 长期依赖的挑战


## 回声状态网络
储层计算循环网络类似于核机器，这是思考它们的一种方式：它们将任意长度
的序列（到时刻 t 的输入历史）映射为一个长度固定的向量（循环状态 h(t)），之后
可以施加一个线性预测算子（通常是一个线性回归）以解决感兴趣的问题。训练准
则就可以很容易地设计为输出权重的凸函数。

？？？ 谱半径？？？

## 渗漏单元和其他多时间尺度的策略
处理长期依赖的一种方法是设计工作在多个时间尺度的模型，使模型的某些部
分在细粒度时间尺度上操作并能处理小细节，而其他部分在粗时间尺度上操作并能
把遥远过去的信息更有效地传递过来。存在多种同时构建粗细时间尺度的策略。这
些策略包括在时间轴增加跳跃连接， “渗漏单元’’ 使用不同时间常数整合信号，并去
除一些用于建模细粒度时间尺度的连接。

### 时间维度的跳跃连接

### 渗漏单元和一系列不同时间尺度

### 删除连接

## 长短期记忆和其他门控 RNN
本文撰写之时，实际应用中最有效的序列模型称为 门控 RNN（ gated RNN）。
包括基于 长短期记忆（ long short-term memory）和基于 门控循环单元（ gated
recurrent unit）的网络。

### LSTM
LSTM 网络比简单的循环架构更易于学习长期依赖。

### 其他门控 RNN
LSTM 架构中哪些部分是真正必须的？还可以设计哪些其他成功架构允许网络
动态地控制时间尺度和不同单元的遗忘行为？


## 优化长期依赖

### 截断梯度

### 外显记忆

神经网络擅长存储隐性知识，但是他们很难记住事实。