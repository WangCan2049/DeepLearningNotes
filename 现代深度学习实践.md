现代深度学习实践
---
反向传播算法，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

实际上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习。



？？？？ 最大似然估计

# 深度学习中的正则化

在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。这些策略被统称为正则化。

我们将正则化定义为 “对学习算法的修改——旨在减少泛化误差而不是训练误差”。

在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常只对权重做惩罚而不对偏置做正则惩罚。


机器学习中许多线性模型，包括线性回归和 PCA，都依赖于对矩阵 X<sup>⊤</sup>X 求逆。

对参数的 L1 惩罚诱导参数稀疏性.

????稀疏有何好处？

