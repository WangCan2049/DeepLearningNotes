优化
---


## 学习和纯优化不同

用于深度模型训练的优化算法与传统的优化算法在几个方面有所不同。 

经验风险最小化很容易导致过拟合。高容量的模型会简单地记住训练集。

我们通常会优化 代理损失函数（ surrogate loss function）。 它是实际损失函数的替代。

 机器学习通常优化代理损失函数，但是在基于提前终止（第 7.8 节）的收敛条件满足时停止。

机器学习通常优化代理损失函数，但是在基于提前终止
（第 7.8 节）的收敛条件满足时停止。

机器学习中的优化算法在计算参数的每一次更新时通常仅使用整个代价函数中一部分项来估计代价函数的期望值。

???最大似然估计？？？

## 批量算法和小批量算法
1.小批量计算比大批量计算，更快，累积起来的效果比一次大批量好。以标准差为例。
2.训练集多为冗余.小批量省时间。

小批量大小的决定因素：
-    更大的批量会计算更精确的梯度估计，但是回报却是小于线性的。
-    极小批量通常难以充分利用多核架构。这促使我们使用一些绝对最小批量，低于这个值的小批量处理不会减少计算时间。
-    如果批量处理中的所有样本可以并行地处理（通常确是如此），那么内存消耗和批量大小会正比。对于很多硬件设施，这是批量大小的限制因素。

## 神经网络优化中的挑战

### 病态
病态问题一般被认为存在于神经网络训练过程中。 病态体现在随机梯度下降会‘‘卡’’ 在某些情况，此时即使很小的更新步长也会增加代价函数。

### 局部极小值
凸优化问题的一个突出特点是其可以简化为寻找一个局部极小点的问题。

如果一个足够大的训练集可以唯一确定一组模型参数，那么该模型被称为可辨认的。

这种不可辨认性被称为 权重空间对称性（ weight space symmetry）。

很多从业者将神经网络优化中的所有困难都归结于局部极小值。我们鼓励从业者要仔细分析特定的问题。
一种能够排除局部极小值是主要问题的检测方法是画出梯度范数随时间的变化。如果梯度范数没有缩小到一个微小的值，那么该问题既不是局部极小值，也不是其他形式的临界点。

### 高原、鞍点和其他平坦区域

？？？？hessian矩阵

### 悬崖和梯度爆炸

### 长期依赖

## 基本算法
### 随机梯度下降
按照数据生成分布抽取 m 个小批量（独立同分布的）样本，通过计算它们梯度均值，我们可以得到梯度的无偏估计。


？？？无偏估计？？？

SGD 算法中的一个关键参数是学习率。

学习率可通过试验和误差来选取，通常最好的选择方法是监测目标函数值随时间变化的学习曲线。

对于大数据集， SGD 只需非常少量样本计算梯度从而实现初始快速更新，远远超过了其缓慢的渐近收敛。

### 动量
动量的主要目的是解决两个问题： Hessian 矩阵的病态条件和随机梯度的方差。

从形式上看， 动量算法引入了变量 v 充当速度角色——它代表参数在参数空间移动的方向和速率。

我们可以将动量算法视为模拟连续时间下牛顿动力学下的粒子。这种物理类比有助于直觉上理解动量和梯度下降算法是如何表现的。

粘性阻力避免了这两个问题——它足够弱，可以使梯度引起的运动直到达到最小，但又足够强，使得坡度不够时可以阻止运动。

### Nesterov 动量

## 参数初始化策略
有些优化算法本质上是非迭代的，只是求解一个解点。有些其它优化算法本质上是迭代的，但是应用于这一类的优化问题时，能在可接受的时间内收敛到可接受的解，并且与初始值无关。 深度学习训练算法通常没有这两种奢侈的性质。 
深度学习模型的训练算法通常是迭代的，因此要求使用者指定一些开始迭代的初始点。

通常来说，最好还是初始化每个单元使其和其他单元计算不同的函数。这或许有助于确保没有输入模式丢失在前向传播的零空间中，没有梯度模式丢失在反向传播的零空间中。

除了这些初始化模型参数的简单常数或随机方法，还有可能使用机器学习初始化模型参数。

## 自适应学习率算法
神经网络研究员早就意识到学习率肯定是难以设置的超参数之一，因为它对模型的性能有显著的影响。

损失通常高度敏感于参数空间中的某些方向，而不敏感于其他。 


### AdaGrad
对于训练深度神经网络模型而言， 从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。 

### RMSProp
RMSProp 使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的 AdaGrad 算法实例。

### Adam
首先，在 Adam 中，动量直接并入了梯度一阶矩（指数加权）的估计。将动量加入 RMSProp 最直观的方法是将动量应用于缩放后的梯度。

## 二阶近似方法
### 牛顿法
与一阶方法相比，二阶方法使用二阶导数改进了优化。最广泛使用的二阶方法是牛顿法。

$$θ^∗ = θ_0 − H^{−1}∇_θJ(θ_0)$$

由于计算量大，只有参数很少的网络才能在实际中用牛顿法训练。

### 共轭梯度
共轭梯度是一种通过迭代下降的 共轭方向（ conjugate directions）以有效避免 Hessian 矩阵求逆计算的方法。

每一个由梯度给定的线搜索方向，都保证正交于上一个线搜索方向。

在共轭梯度法中，我们寻求一个和先前线搜索方向 共轭（conjugate）的搜索方向，即它不会撤销该方向上的进展。

$$\textbf{d}t = ∇_θ J(\textbf{θ}) + β_t \textbf{d}_{t−1}$$

对于二次曲面而言，共轭方向确保梯度沿着前一方向大小不变。因此，我们在前一方向上仍然是极小值。

### BFGS
Broyden-Fletcher-Goldfarb-Shanno（ BFGS）算法具有牛顿法的一些优点，但没有牛顿法的计算负担。在这方面， BFGS 和 CG 很像。然而， BFGS 使用了一个更直接的方法近似牛顿更新。

拟牛顿法所采用的方法（ BFGS 是其中最突出的）是使用矩阵 $\textbf{M}_t$ 近似逆，迭代地低秩更新精度以更好地近似 $\textbf{H}^{−1}$。

存 储 受 限 的 BFGS（或 L-BFGS） 通 过 避 免 存 储 完 整 的 Hessian 逆 近 似 M，BFGS 算法的存储代价可以显著降低。

## 优化策略和元算法
### 批标准化
批标准化提出了一种几乎可以重参数化所有深度网络的优雅方法。重参数化显著减少了多层之间协调更新的问题。 批标准化可应用于网络的任何输入层或隐藏层。

批标准化 (Ioffe and Szegedy, 2015) 是优化深度神经网络中最激动人心的最新创新之一。实际上它并不是一个优化算法，而是一个自适应的重参数化的方法，试图解决训练非常深的模型的困难。

非常深的模型会涉及多个函数或层组合。在其他层不改变的假设下，梯度用于如何更新每一个参数。在实践中，我们同时更新所有层。

？？？设计矩阵？？？

### 坐标下降
这种做法被称为 坐标下降（ coordinate descent），因为我们一次优化一个坐标。更一般地， 块坐标下降（ blockcoordinate descent）是指对于某个子集的变量同时最小化。术语 “坐标下降’’ 通常既指块坐标下降，也指严格的单个坐标下降。

### Polyak 平均
基本想法是，优化算法可能会来回穿过山谷好几次而没经过山谷底部附近的点。尽管两边所有位置的均值应比较接近谷底。

### 监督预训练
训练模型来求解一个简化的问题，然后转移到最后的问题，有时也会更有效些。这些在直接训练目标模型求解目标问题之前，训练简单模型求解简化问题的方法统称为 预训练（ pretraining）。

贪心算法（ greedy algorithm）将问题分解成许多部分，然后独立地在每个部分求解最优值。令人遗憾的是，结合各个最佳的部分不能保证得到一个最佳的完整解。

个人理解：纵向切割问题。

为什么贪心监督预训练会有帮助呢？最初由 Bengio et al. (2007d) 提出的假说是，其有助于更好地指导深层结构的中间层的学习。一般情况下， 预训练对于优化和泛化都是有帮助的。

### 设计有助于优化的模型
改进优化的最好方法并不总是改进优化算法。相反， 深度模型中优化的许多改进来自于设计易于优化的模型。

在实践中， 选择一族容易优化的模型比使用一个强大的优化算法更
重要。 

现代神经网络的设计方案旨在使其局部梯度信息合理地对应着移向一个遥远的解。

### 延拓法和课程学习
延拓法（ continuation method）是一族通过挑选初始点使优化更容易的方法，以确保局部优化花费大部分时间在表现良好的空间。 延拓法的背后想法是构造一系列具有相同参数的目标函数。

传统上， 延拓法主要用来克服局部极小值的问题。具体地，它被设计来在有很多局部极小值的情况下，求解一个全局最小点。这些连续方法会通过 ‘‘模糊’’ 原来的代价函数来构建更容易的代价函数。


