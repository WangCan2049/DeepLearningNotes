数值计算
---

## 上溢和下溢

## 病态条件

## 基于梯度的优化方法

记 $x^* = \arg \min f(x)$

要最大化的函数称为目标函数objective function
要最小化的函数称为代价函数cost function 损失函数 loss function 或误差函数 error function

f'(x)=0的点为临界点critical funcition或驻点 stationary point

既不是局部极小点，也不是局部极大点的临界点，称之为鞍点saddle point

针对具有多维输入的函数，我们需要用到 偏导数（ partial derivative）的概念。
偏导数 $\frac{\partial}{\partial x_i} f(\textbf{x})$ 衡量点 **x** 处只有 $x_i$ 增加时 f(**x**) 如何变化。 梯度（ gradient）是相对一个向量求导的导数 f 的导数是包含所有偏导数的向量，记为 ∇xf(x)。 梯度的第i 个元素是 f 关于 $x_i$ 的偏导数。在多维情况下， 临界点是梯度中所有元素都为零的点。


在 **u**（单位向量）方向的 方向导数（ directional erivative）是函数 f 在 **u** 方向的斜率。换句话说， 方向导数是函数 f(**x** + α**u**) 关于 α 的导数（在 α = 0 时取得）。
使用链式法则，我们可以看到当 α = 0 时， $\frac{\partial}{\partial α} f(\textbf{x} + α \textbf{u}) = \textbf{u}^⊤ ∇_x f(\textbf{x})$

### 方向导数推导过程
http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx

梯度下降法：最小化一个函数时，将x往导数的反方向移动一小步来较小f(x).


最速下降新的点为

$$x' = x - ε ∇_x f(x)$$

## jacobian矩阵和hessian矩阵
计算输入和输出都为向量的函数的所有偏导数.包含所有这样偏导数的矩阵被成为jaconbian矩阵。
具体来说，有这样一个函数: $f:\mathbb{R}^m \to \mathbb{R}^n$, f的jaconbian矩阵$J \in \mathbb{R}^{m*n}$定义为$J_{i,j} = \frac{\partial }{\partial x_j}f(x)_i$

二阶导数：导数的导数。可以人为，二阶导数是对曲率的衡量。


### 泰勒展开
https://en.wikipedia.org/wiki/Taylor_series

f(x)在复数a处的泰勒展开：

$$\sum_{n=0}^{\infty} \frac {f^{(n)(a)}} {n!} (x-a)^n$$


针对有多维输入的函数，我们需要用到偏导数（partial devirative）的概念。偏导数 $\frac {\partial} {\partial x_i} f(x)$衡量点x处只有$x_i$增加时f(x)如何变化。

在**u**(单位向量)方向的**方向导数**(directional derivate)时f在**u**方向的斜率。


### Hessian 矩阵
二阶偏导数矩阵

### 牛顿法与hessian矩阵

仅使用梯度信息的算法为一阶优化算法，使用hessian矩阵的优化算法为二阶优化算法，如牛顿法。


## 约束优化
在特定集合中找f(x)最大值、最小值。



# 疑惑点

方向导数

